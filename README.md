# MachineLearning_OpenAIGym
We attempt to train a Deep Q-Network (DQN) agent to simulate the OpenAI Gym’s CarRacing-v2 environment . The DQN agent adapts a Convolutional Neural Network (CNN) to learn a Q-value function that estimates the expected future reward for each action in a given state. 
The agent then uses an epsilon-greedy policy to choose the actions based on the Q-values. The training process involves replaying the experiences stored in a memory buffer and then using the Bellman equation to update the Q-values. The approach we aimed includes some utility functions for processing images and generating state frames, as well as code for setting up a virtual display to render the game environment.

We import the necessary packages and installs some additional dependencies via pip and apt-get. We define a CarRacingDQNAgent class that contain the DQN agent's properties and methods. The agent is initialized with different hyperparameters, such as the action space, frame stack size, memory size, discount rate, and learning rate. 

The build_model method creates a CNN with several layers that takes in a stack of grayscale frames and outputs a Q-value for each action in the action space. The replay method samples experiences from the memory buffer, calculates the target Q-values, and trains the model using mini-batch gradient descent. The ‘act method’ selects an action to take based on the current state and the agent's exploration/exploitation strategy. The ‘memorize method’ stores the agent's experience in the memory buffer. The update_target_model method updates the weights of the target network, which is used to compute the target Q-values during training.

The main function sets up the CarRacing-v2 environment via the function gym.make() which will initialize the agent, and trains it using a loop that runs for a fixed number of episodes and get the accuracies of the reward attained. The loop repeatedly samples a state from the environment, uses the agent to select an action, applies the action to the environment, stores the resulting experience in the memory buffer, and updates the agent's Q-values. The loop also saves the model weights and renders the game environment at regular intervals. The script outputs the average reward per episode and the total training time.
